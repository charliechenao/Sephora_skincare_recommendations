{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup \n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--disable-gpu')\n",
    "DRIVER_PATH = 'chromedriver'\n",
    "driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Getting a list of item page URLs from the product list page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to determine how many pages are there in total.\n",
    "start_url = \"https://www.sephora.com/shop/clean-skin-care?pageSize=300&currentPage=1\"\n",
    "\n",
    "# This function will scrape the page at starting_url\n",
    "# and return an integer representing the last pagination number.\n",
    "def find_last_page_number(starting_url):\n",
    "    # request the html using the url, using selenium to take care of the javascript rendering stuff\n",
    "    driver.get(starting_url)\n",
    "    # scroll to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    last_page_button = soup.find_all(\"button\", class_=\"css-1lk9n5p eanm77i0\")[-1]\n",
    "    return int(last_page_button.text)\n",
    "\n",
    "# This function will scrape and return a list of all products' urls on page_url.\n",
    "def get_product_urls(page_url):\n",
    "    # request the product list page using page_url\n",
    "    driver.get(page_url)\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    # using selenium, we slow-scroll to the bottom to lazy-load all the products\n",
    "    for i in range(1, total_height, 5):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "    # gotta do this twice to account for the last few products\n",
    "    new_total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(total_height, new_total_height, 5):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "    # once all products are loaded, we can easily parse the URLs\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    group_elements = soup.find_all(\"div\", class_=\"css-dkxsdo\")\n",
    "    result = []\n",
    "    # for each group, iterate over all 12 of its child elements (individual products)\n",
    "    for g in group_elements:\n",
    "        child_products = g.findChildren(\"div\", class_=\"css-12egk0t\", recursive=False)\n",
    "        for c in child_products:\n",
    "            product = c.findChildren(\"a\", recursive=False)\n",
    "            result.append(\"https://www.sephora.com\" + product[0][\"href\"])\n",
    "    return result\n",
    "\n",
    "# This function combines find_last_page_number and get_product_urls to return a list of \n",
    "# all products across all the pages.\n",
    "def get_all_product_urls(start_url):\n",
    "    last_page_number = find_last_page_number(start_url)\n",
    "    last_page_number = 1\n",
    "    result = []\n",
    "    for i in range(1, last_page_number + 1):\n",
    "        # build out the URL of the current page by changing the currentPage=X part of the URL\n",
    "        current_url = start_url[:-1] + str(i)\n",
    "        current_product_urls = get_product_urls(current_url)\n",
    "        result = result + current_product_urls\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Scrape each product page individually to retrieve the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will scrape the page at starting_url\n",
    "# and return an integer representing the last pagination number for the reviews.\n",
    "def find_last_review_page_number(starting_url):\n",
    "    # request the html using the url, using selenium to take care of the javascript rendering stuff\n",
    "    driver.get(starting_url)\n",
    "    # slowly scroll to the bottom so that the pagination bar fully loads\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    # using selenium, we slow-scroll to the bottom to lazy-load all the products\n",
    "    for i in range(1, total_height, 5):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    last_page_button = soup.find_all(\"button\", class_=\"css-exi524\")[-1]\n",
    "    return int(last_page_button.text)\n",
    "\n",
    "# this function requires a beautiful soup document of the sephora product page\n",
    "# the function returns (product brand,name)\n",
    "def scrape_product_name(doc):\n",
    "    product_container = doc.find_all('h1', class_='css-11zrkxf e65zztl0')[0]\n",
    "    product_brand = product_container.findChild(\"a\").text \n",
    "    product_name = product_container.findChild(\"span\").text\n",
    "    return (product_brand, product_name)\n",
    "\n",
    "\n",
    "# scrape review section\n",
    "# this function requires beautiful soup version of the product page\n",
    "# the function returns [beautifulsoup elements] (each element represent a review)\n",
    "# NOTE: this only finds the review section for 1 single page of reviews\n",
    "def scrape_review_section(doc):\n",
    "    return doc.find_all('div', class_=\"css-13o7eu2 eanm77i0\")\n",
    "\n",
    "\n",
    "# function input: single beautifulsoup element representing single review container\n",
    "# function output: extract (product_brand, product_name, skin tone, skin type, star)\n",
    "# if skin tone and skin type were empty, the function will return empty string \n",
    "def extract_review_properties(product_brand, product_name, review):\n",
    "    star = int(review.find('div', class_='css-4qxrld')['aria-label'][0])\n",
    "    skin_attributes = review.find('div', class_=\"css-z04cd8 eanm77i0\").findChild(\"span\").text\n",
    "    # split this long string of combined skin attributes into a list of attributes i.e. [eye color, hair color, skin tone, skin type]\n",
    "    skin_attributes_list = skin_attributes.split(\", \")\n",
    "    # extract the elements which contain the word \"skin\" -- ideally we only want the skin type and skin tone attributes\n",
    "    filtered_skin_attributes_list = [i for i in skin_attributes_list if \"skin\" in i]\n",
    "    skin_tone = \"\"\n",
    "    skin_type = \"\"\n",
    "    if len(filtered_skin_attributes_list) == 0: # we do nothing here, leave skin type and skin tone as empty string\n",
    "        pass\n",
    "    if len(filtered_skin_attributes_list) == 1: # user only inputed skin type OR skin tone\n",
    "        if \"skin tone\" in filtered_skin_attributes_list[0]:\n",
    "            skin_tone = filtered_skin_attributes_list[0]\n",
    "        else:\n",
    "            skin_type = filtered_skin_attributes_list[0]\n",
    "        \n",
    "    else: # user gave both skin type AND skin tone\n",
    "        # sephora inputs skin tone first before skin type\n",
    "        skin_tone = filtered_skin_attributes_list[0]\n",
    "        skin_type = filtered_skin_attributes_list[1]\n",
    "    return (product_brand, product_name, skin_tone, skin_type, star)\n",
    "\n",
    "\n",
    "# function purpose: extract the properties from all the reviews on a single page of reviews\n",
    "# input: the beautifulsoup version of the product page\n",
    "# output: the properties from all the reviews [(skin_tone, skin_type, star)]\n",
    "# remove review that doesn't contain skin type or skin tone\n",
    "# NOTE: this only finds the review section for 1 single page of reviews\n",
    "def extract_reviews_properties(doc):\n",
    "    reviews = scrape_review_section(doc)\n",
    "    product_brand, product_name = scrape_product_name(doc)\n",
    "    reviews_properties = []\n",
    "    for r in reviews:\n",
    "        review_properties = extract_review_properties(product_brand, product_name, r)\n",
    "        if review_properties[0] != '' and review_properties[1] != '':\n",
    "            reviews_properties.append(review_properties)        \n",
    "    return reviews_properties\n",
    "\n",
    "# this function will scrape all the reviews across all the paginated review pages\n",
    "# input: url of the starting page\n",
    "# output: properties from all the reviews across all paginations [(product_brand, product_name, skin_tone, skin_type, star)]\n",
    "def extract_paginated_reviews_properties(start_url):\n",
    "    result = []\n",
    "    total_pages = find_last_review_page_number(start_url)\n",
    "    for i in range(6):\n",
    "        # scrape that page\n",
    "        doc = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        result = result + extract_reviews_properties(doc)\n",
    "        # then press next page only if we are not already on the last page\n",
    "        if i != total_pages - 1:\n",
    "            driver.find_element_by_class_name(\"css-2anst8\").click()\n",
    "    return result      \n",
    "\n",
    "# this function scrapes all the products and all their reviews, saves the output to a csv\n",
    "# input: url of the product list page, csv file name\n",
    "# output: None, but will save a CSV \n",
    "def pipeline(product_list_url, csv_file_name):\n",
    "    products_list = get_all_product_urls(product_list_url)\n",
    "    for p in products_list:\n",
    "        reviews = extract_paginated_reviews_properties(p)\n",
    "        with open(csv_file_name,'a') as out:\n",
    "            csv_out = csv.writer(out)\n",
    "            csv_out.writerow(['product_brand','product_name', \"skin_tone\", \"skin_type\", \"ratings\"])\n",
    "            for r in reviews:\n",
    "                csv_out.writerow(r)\n",
    "        out.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(\"https://www.sephora.com/shop/clean-skin-care?currentPage=1\", \"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
